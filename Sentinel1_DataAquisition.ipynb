{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6BXESUQ0/9jOd6wA/zPF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kazuma2002/OpenScienceDataChallenge/blob/main/Sentinel1_DataAquisition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_7nln51mERt"
      },
      "outputs": [],
      "source": [
        "# Supress Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import ipyleaflet\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "import seaborn as sns\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Feature Engineering\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Planetary Computer Tools\n",
        "import pystac\n",
        "import pystac_client\n",
        "import odc\n",
        "from pystac_client import Client\n",
        "from pystac.extensions.eo import EOExtension as eo\n",
        "from odc.stac import stac_load\n",
        "import planetary_computer as pc\n",
        "\n",
        "#Please pass your API key here\n",
        "pc.settings.set_subscription_key('c3ed0e9c76f44014a77ef43b454f6747')\n",
        "\n",
        "# Others\n",
        "import requests\n",
        "import rich.table\n",
        "from itertools import cycle\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "tqdm_notebook.pandas()\n",
        "\n",
        "#Additionals\n",
        "!pip install mlxtend\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import multiprocessing\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import xarray as xr\n",
        "from skimage import exposure\n",
        "from skimage.feature import graycomatrix, graycoprops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crop_yield_data = pd.read_csv(\"Crop_Yield_Data_challenge_2.csv\")\n",
        "crop_yield_data.head()"
      ],
      "metadata": {
        "id": "wafnZ7YrnKHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentinel-1"
      ],
      "metadata": {
        "id": "x67PddxNnOfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentinel_data(longitude, latitude, season,assests):\n",
        "\n",
        "    bands_of_interest = assests\n",
        "    if season == 'SA':\n",
        "        time_slice = \"2022-05-01/2022-08-31\"\n",
        "    if season == 'WS':\n",
        "        time_slice = \"2022-01-01/2022-04-30\"\n",
        "\n",
        "    vv_list = []\n",
        "    vh_list = []\n",
        "    vv_by_vh_list = []\n",
        "\n",
        "    bbox_of_interest = [longitude , latitude, longitude, latitude]\n",
        "    time_of_interest = time_slice\n",
        "\n",
        "    catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
        "    search = catalog.search(collections=[\"sentinel-1-rtc\"], bbox=bbox_of_interest, datetime=time_of_interest)\n",
        "    items = list(search.get_all_items())\n",
        "    item = items[0]\n",
        "    items.reverse()\n",
        "\n",
        "    data = stac_load([items[1]],bands=bands_of_interest, patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "\n",
        "    for item in items:\n",
        "        data = stac_load([item], bands=bands_of_interest, patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "        if(data['vh'].values[0][0]!=-32768.0 and data['vv'].values[0][0]!=-32768.0):\n",
        "            data = data.where(~data.isnull(), 0)\n",
        "            vh = data[\"vh\"].astype(\"float64\")\n",
        "            vv = data[\"vv\"].astype(\"float64\")\n",
        "            vv_list.append(np.median(vv))\n",
        "            vh_list.append(np.median(vh))\n",
        "            vv_by_vh_list.append(np.median(vv)/np.median(vh))\n",
        "\n",
        "    return vv_list, vh_list, vv_by_vh_list"
      ],
      "metadata": {
        "id": "Rcr7nHygnMA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Get Sentinel-1-RTC Data\n",
        "def get_sentinel_data_for_record(record):\n",
        "    # Extract the longitude, latitude, and season from the record\n",
        "    longitude = record['Longitude']\n",
        "    latitude = record['Latitude']\n",
        "    season = record['Season(SA = Summer Autumn, WS = Winter Spring)']\n",
        "\n",
        "    assests = ['vh', 'vv']\n",
        "    vh, vv, vv_by_vh = get_sentinel_data(longitude, latitude, season, assests)\n",
        "\n",
        "    return (vh, vv, vv_by_vh)\n",
        "\n",
        "pool = multiprocessing.Pool()\n",
        "results = []\n",
        "for result in tqdm(pool.imap(get_sentinel_data_for_record, crop_yield_data.to_dict('records')), total=len(crop_yield_data)):\n",
        "    results.append(result)\n",
        "pool.close()\n",
        "\n",
        "vh = [x[0] for x in results]\n",
        "vv = [x[1] for x in results]\n",
        "vv_by_vh = [x[2] for x in results]\n",
        "vh_vv_data = pd.DataFrame(list(zip(vh, vv, vv_by_vh)), columns=['vh_list', 'vv_list', 'vv/vh_list'])\n",
        "#vh_vv_data = pd.read_csv(\"Sentinel1_data.csv\")"
      ],
      "metadata": {
        "id": "FEEMSp6enQpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate rvi using Sentinel-1 Data\n",
        "def calculate_rvi(mean_vh, mean_vv):\n",
        "    dop = mean_vv / (mean_vv + mean_vh)\n",
        "    m = 1 - dop\n",
        "    rvi = np.sqrt(dop) * (4 * mean_vh) / (mean_vv + mean_vh)\n",
        "    return rvi"
      ],
      "metadata": {
        "id": "HqF3z2RhnTAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ordinal_distribution(data, dx=3, dy=1, taux=1, tauy=1, return_missing=False, tie_precision=None):\n",
        "    def setdiff(a, b):\n",
        "\n",
        "        a = np.asarray(a).astype('int64')\n",
        "        b = np.asarray(b).astype('int64')\n",
        "\n",
        "        _, ncols = a.shape\n",
        "\n",
        "        dtype={'names':['f{}'.format(i) for i in range(ncols)],\n",
        "            'formats':ncols * [a.dtype]}\n",
        "\n",
        "        C = np.setdiff1d(a.view(dtype), b.view(dtype))\n",
        "        C = C.view(a.dtype).reshape(-1, ncols)\n",
        "\n",
        "        return(C)\n",
        "\n",
        "    try:\n",
        "        ny, nx = np.shape(data)\n",
        "        data   = np.array(data)\n",
        "    except:\n",
        "        nx     = np.shape(data)[0]\n",
        "        ny     = 1\n",
        "        data   = np.array([data])\n",
        "\n",
        "    if tie_precision is not None:\n",
        "        data = np.round(data, tie_precision)\n",
        "\n",
        "    partitions = np.concatenate(\n",
        "        [\n",
        "            [np.concatenate(data[j:j+dy*tauy:tauy,i:i+dx*taux:taux]) for i in range(nx-(dx-1)*taux)]\n",
        "            for j in range(ny-(dy-1)*tauy)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    symbols = np.apply_along_axis(np.argsort, 1, partitions)\n",
        "    symbols, symbols_count = np.unique(symbols, return_counts=True, axis=0)\n",
        "\n",
        "    probabilities = symbols_count/len(partitions)\n",
        "\n",
        "    if return_missing==False:\n",
        "        return symbols, probabilities\n",
        "\n",
        "    else:\n",
        "        all_symbols   = list(map(list,list(itertools.permutations(np.arange(dx*dy)))))\n",
        "        miss_symbols  = setdiff(all_symbols, symbols)\n",
        "        symbols       = np.concatenate((symbols, miss_symbols))\n",
        "        probabilities = np.concatenate((probabilities, np.zeros(miss_symbols.__len__())))\n",
        "\n",
        "        return symbols, probabilities"
      ],
      "metadata": {
        "id": "UoyV-hDOnVSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def permutation_entropy(data, dx=3, dy=1, taux=1, tauy=1, base=2, normalized=True, probs=False, tie_precision=None):\n",
        "    if not probs:\n",
        "        _, probabilities = ordinal_distribution(data, dx, dy, taux, tauy, return_missing=False, tie_precision=tie_precision)\n",
        "    else:\n",
        "        probabilities = np.asarray(data)\n",
        "        probabilities = probabilities[probabilities>0]\n",
        "\n",
        "    if normalized==True and base in [2, '2']:\n",
        "        smax = np.log2(float(np.math.factorial(dx*dy)))\n",
        "        s    = -np.sum(probabilities*np.log2(probabilities))\n",
        "        return s/smax\n",
        "\n",
        "    elif normalized==True and base=='e':\n",
        "        smax = np.log(float(np.math.factorial(dx*dy)))\n",
        "        s    = -np.sum(probabilities*np.log(probabilities))\n",
        "        return s/smax\n",
        "\n",
        "    elif normalized==False and base in [2, '2']:\n",
        "        return -np.sum(probabilities*np.log2(probabilities))\n",
        "    else:\n",
        "        return -np.sum(probabilities*np.log(probabilities))"
      ],
      "metadata": {
        "id": "L5svaZIinXiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_stastical_features(dataframe):\n",
        "    features_list = []\n",
        "    for index, row in dataframe.iterrows():\n",
        "\n",
        "        min_vv = min(row[0])\n",
        "        max_vv = max(row[0])\n",
        "        range_vv = max_vv - min_vv\n",
        "        mean_vv = np.mean(row[0])\n",
        "        correlation_vv = sm.tsa.acf(row[0])[1]\n",
        "        permutation_entropy_vv = permutation_entropy(row[0], dx=6,base=2, normalized=True)\n",
        "\n",
        "        min_vh = min(row[1])\n",
        "        max_vh = max(row[1])\n",
        "        range_vh = max_vh - min_vh\n",
        "        mean_vh = np.mean(row[1])\n",
        "        correlation_vh = sm.tsa.acf(row[1])[1]\n",
        "        permutation_entropy_vh = permutation_entropy(row[1], dx=6, base=2, normalized=True)\n",
        "\n",
        "        min_vv_by_vh = min(row[2])\n",
        "        max_vv_by_vh = max(row[2])\n",
        "        range_vv_by_vh = max_vv_by_vh - min_vv_by_vh\n",
        "        mean_vv_by_vh = np.mean(row[2])\n",
        "        correlation_vv_by_vh = sm.tsa.acf(row[2])[1]\n",
        "        permutation_entropy_vv_by_vh = permutation_entropy(row[2], dx=6, base=2, normalized=True)\n",
        "\n",
        "        rvi = calculate_rvi(mean_vh, mean_vv)\n",
        "\n",
        "        backscatter_coefficient = 10 * np.log10((mean_vv ** 2 + mean_vh ** 2) / 2)\n",
        "        polarization = np.sqrt(mean_vv ** 2 + mean_vh ** 2)\n",
        "\n",
        "        features_list.append([min_vv, max_vv, range_vv, mean_vv, correlation_vv, permutation_entropy_vv,\n",
        "                          min_vh, max_vh, range_vh,  mean_vh, correlation_vh, permutation_entropy_vh,\n",
        "                          min_vv_by_vh,  max_vv_by_vh, range_vv_by_vh, mean_vv_by_vh, correlation_vv_by_vh,\n",
        "                          permutation_entropy_vv_by_vh, rvi, backscatter_coefficient, polarization])\n",
        "    return features_list"
      ],
      "metadata": {
        "id": "ALkASnctnZMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Statistical Features for VV,VH and VV/VH and creating a dataframe\n",
        "features = generate_stastical_features(vh_vv_data)\n",
        "features_data = pd.DataFrame(features, columns = ['min_vv', 'max_vv', 'range_vv', 'mean_vv', 'correlation_vv',\n",
        "                                                  'permutation_entropy_vv','min_vh', 'max_vh', 'range_vh', 'mean_vh',\n",
        "                                                  'correlation_vh', 'permutation_entropy_vh', 'min_vv_by_vh',\n",
        "                                                  'max_vv_by_vh', 'range_vv_by_vh', 'mean_vv_by_vh',\n",
        "                                                  'correlation_vv_by_vh', 'permutation_entropy_vv_by_vh', 'rvi',\n",
        "                                                  'backscatter_coefficient', 'polarization'])"
      ],
      "metadata": {
        "id": "ePXvKjKbneLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentinel-2"
      ],
      "metadata": {
        "id": "GOolQXU2njb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def colorize(xx, colormap):\n",
        "    return xr.DataArray(colormap[xx.data], coords=xx.coords, dims=(*xx.dims, \"band\"))"
      ],
      "metadata": {
        "id": "0jX_veKKngI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentinel2_data(longitude, latitude, season, assets):\n",
        "    '''\n",
        "    Returns a list of RGB, NIR values for a given latitude and longitude over a given time period (based on the season)\n",
        "    Attributes:\n",
        "    bbox - bounding box\n",
        "    time_range - time_of_interest\n",
        "    '''\n",
        "    bands_of_interest = assets\n",
        "    if season == 'SA':\n",
        "        time_slice = \"2022-05-01/2022-08-31\"\n",
        "    elif season == 'WS':\n",
        "        time_slice = \"2022-01-01/2022-04-30\"\n",
        "\n",
        "    red_list = []\n",
        "    green_list = []\n",
        "    blue_list = []\n",
        "    nir_list = []\n",
        "    swir_list = []\n",
        "    ndvi_list = []\n",
        "    ndwi_list = []\n",
        "    ndmi_list = []\n",
        "\n",
        "    box_size_deg = 0.0004 # Surrounding box in degrees\n",
        "    min_lon = longitude-box_size_deg/2\n",
        "    min_lat = latitude-box_size_deg/2\n",
        "    max_lon = longitude+box_size_deg/2\n",
        "    max_lat = latitude+box_size_deg/2\n",
        "    bounds = (min_lon, min_lat, max_lon, max_lat)\n",
        "\n",
        "    catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
        "    search = catalog.search(\n",
        "        collections=[\"sentinel-2-l2a\"],\n",
        "        bbox=bounds,  # Pass the tuple instead of the Polygon object\n",
        "        datetime=time_slice)\n",
        "    items = list(search.get_all_items())\n",
        "\n",
        "    resolution = 10  # meters per pixel\n",
        "    scale = resolution / 111320.0 # degrees per pixel for CRS:4326\n",
        "\n",
        "    xx = stac_load(\n",
        "            items,\n",
        "            bands=[\"red\", \"green\", \"blue\", \"nir\", \"swir16\", \"SCL\"],\n",
        "            crs=\"EPSG:4326\", # Latitude-Longitude\n",
        "            resolution=scale, # Degrees\n",
        "            chunks={\"x\": 2048, \"y\": 2048},\n",
        "            dtype=\"uint16\",\n",
        "            patch_url=pc.sign,\n",
        "            bbox=bounds)\n",
        "\n",
        "    # Create a colormap to display the SCL pixel classifications\n",
        "    scl_colormap = np.array(\n",
        "        [\n",
        "            [252,  40, 228, 255],  # 0  - NODATA - MAGENTA\n",
        "            [255,   0,   4, 255],  # 1  - Saturated or Defective - RED\n",
        "            [0  ,   0,   0, 255],  # 2  - Dark Areas - BLACK\n",
        "            [97 ,  97,  97, 255],  # 3  - Cloud Shadow - DARK GREY\n",
        "            [3  , 139,  80, 255],  # 4  - Vegetation - GREEN\n",
        "            [192, 132,  12, 255],  # 5  - Bare Ground - BROWN\n",
        "            [21 , 103, 141, 255],  # 6  - Water - BLUE\n",
        "            [117,   0,  27, 255],  # 7  - Unclassified - MAROON\n",
        "            [208, 208, 208, 255],  # 8  - Cloud - LIGHT GREY\n",
        "            [244, 244, 244, 255],  # 9  - Definitely Cloud - WHITE\n",
        "            [195, 231, 240, 255],  # 10 - Thin Cloud - LIGHT BLUE\n",
        "            [222, 157, 204, 255],  # 11 - Snow or Ice - PINK\n",
        "        ],\n",
        "        dtype=\"uint8\",)\n",
        "\n",
        "    # Load SCL band, then convert to RGB using color scheme above\n",
        "    scl_rgba = colorize(xx.SCL.compute(), scl_colormap)\n",
        "    # Create a mask for no data, saturated data, clouds, cloud shadows, and water\n",
        "    cloud_mask = \\\n",
        "        (xx.SCL != 0) & \\\n",
        "        (xx.SCL != 1) & \\\n",
        "        (xx.SCL != 3) & \\\n",
        "        (xx.SCL != 6) & \\\n",
        "        (xx.SCL != 8) & \\\n",
        "        (xx.SCL != 9) & \\\n",
        "        (xx.SCL != 10)\n",
        "    # Apply cloud mask ... NO Clouds, NO Cloud Shadows and NO Water pixels\n",
        "    # All masked pixels are converted to \"No Data\" and stored as 16-bit integers\n",
        "    cleaned_data = xx.where(cloud_mask).astype(\"uint16\")\n",
        "    # Load SCL band, then convert to RGB using color scheme above\n",
        "    scl_rgba_clean = colorize(cleaned_data.SCL.compute(), scl_colormap)\n",
        "\n",
        "    # Calculate the mean of the data across the sample region and then NDVI\n",
        "    # Perform this calculation for the unfiltered and cloud-filtered (clean) datasets\n",
        "    mean_unfiltered = xx.mean(dim=['longitude','latitude']).compute()\n",
        "    ndvi_mean = (mean_unfiltered.nir-mean_unfiltered.red)/(mean_unfiltered.nir+mean_unfiltered.red)\n",
        "    mean_clean = cleaned_data.mean(dim=['longitude','latitude']).compute()\n",
        "    ndvi_mean_clean = (mean_clean.nir-mean_clean.red)/(mean_clean.nir+mean_clean.red)\n",
        "    ndwi = (mean_clean.green - mean_clean.nir) / (mean_clean.green + mean_clean.nir)\n",
        "    ndmi = (mean_clean.nir - mean_clean.swir16) / (mean_clean.nir + mean_clean.swir16)\n",
        "\n",
        "    r = xx[\"red\"].values\n",
        "    g = xx[\"green\"].values\n",
        "    b = xx[\"blue\"].values\n",
        "    nir = xx[\"nir\"].values\n",
        "    swir = xx[\"swir16\"].values\n",
        "\n",
        "    # Apply cloud mask\n",
        "    cloud_mask = np.isin(xx.SCL, [3, 8, 9])\n",
        "    r = np.where(cloud_mask, np.nan, r)\n",
        "    g = np.where(cloud_mask, np.nan, g)\n",
        "    b = np.where(cloud_mask, np.nan, b)\n",
        "    nir = np.where(cloud_mask, np.nan, nir)\n",
        "    swir = np.where(cloud_mask, np.nan, swir)\n",
        "\n",
        "    # Apply exposure adjustment\n",
        "    r = exposure.adjust_log(r, gain=2)\n",
        "    g = exposure.adjust_log(g, gain=2)\n",
        "    b = exposure.adjust_log(b, gain=2)\n",
        "    nir = exposure.adjust_log(nir, gain=2)\n",
        "    swir = exposure.adjust_log(swir, gain=2)\n",
        "\n",
        "    r_mean = np.nanmean(r)\n",
        "    g_mean = np.nanmean(g)\n",
        "    b_mean = np.nanmean(b)\n",
        "    nir_mean = np.nanmean(nir)\n",
        "    swir_mean = np.nanmean(swir)\n",
        "\n",
        "    red_list.append(r_mean)\n",
        "    green_list.append(g_mean)\n",
        "    blue_list.append(b_mean)\n",
        "    nir_list.append(nir_mean)\n",
        "    swir_list.append(swir_mean)\n",
        "    ndvi_list.append(ndvi_mean_clean)\n",
        "    ndwi_list.append(ndwi)\n",
        "    ndmi_list.append(ndmi)\n",
        "\n",
        "    return red_list, green_list, blue_list, nir_list, swir_list, ndvi_list, ndwi_list, ndmi_list"
      ],
      "metadata": {
        "id": "SjzwEliank5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to be used in multiprocessing\n",
        "def get_sentinel2_data_for_bbox(record):\n",
        "    latitude = record['Latitude']\n",
        "    longitude = record['Longitude']\n",
        "    season = record['Season(SA = Summer Autumn, WS = Winter Spring)']\n",
        "    assets = [\"red\", \"green\", \"blue\", \"nir\", \"swir16\", \"SCL\"]\n",
        "    red_list, green_list, blue_list, nir_list, swir_list, ndvi_list, ndwi_list, ndmi_list = get_sentinel2_data(longitude, latitude, season, assets)\n",
        "    return red_list, green_list, blue_list, nir_list, swir_list, ndvi_list, ndwi_list, ndmi_list\n",
        "\n",
        "# Create a multiprocessing pool and apply the function to each record in the crop_yield_data DataFrame\n",
        "pool = multiprocessing.Pool()\n",
        "results = []\n",
        "for result in tqdm(pool.imap(get_sentinel2_data_for_bbox, crop_yield_data.to_dict('records')), total=len(crop_yield_data)):\n",
        "    results.append(result)\n",
        "pool.close()"
      ],
      "metadata": {
        "id": "o9KsghmtnnO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the RGB and NIR lists from the results\n",
        "\n",
        "red_list = [x[0] for x in results]\n",
        "green_list = [x[1] for x in results]\n",
        "blue_list = [x[2] for x in results]\n",
        "nir_list = [x[3] for x in results]\n",
        "swir_list = [x[4] for x in results]\n",
        "ndvi_list = [x[5] for x in results]\n",
        "ndwi_list = [x[6] for x in results]\n",
        "ndmi_list = [x[7] for x in results]"
      ],
      "metadata": {
        "id": "ZtzefkW8nqD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract features from Sentinel-2 Data\n",
        "def extract_features(red_list, green_list, blue_list, nir_list, swir_list, ndvi_list, ndwi_list, ndmi_list):\n",
        "    features2 = []\n",
        "    for i in range(max(len(red_list),len(green_list),len(blue_list),len(nir_list),len(ndvi_list),len(ndwi_list))):\n",
        "        red = red_list[i]\n",
        "        green = green_list[i]\n",
        "        blue = blue_list[i]\n",
        "        nir = nir_list[i]\n",
        "        swir = swir_list[i]\n",
        "        ndvi = ndvi_list[i]\n",
        "        ndwi = ndwi_list[i]\n",
        "        ndmi = ndmi_list[i]\n",
        "\n",
        "        r_mean = np.mean(red)\n",
        "        g_mean = np.mean(green)\n",
        "        b_mean = np.mean(blue)\n",
        "        nir_mean = np.mean(nir)\n",
        "        swir_mean = np.mean(swir)\n",
        "\n",
        "        red_mean = (2 * r_mean + g_mean) / 3\n",
        "        blue_mean = (2 * b_mean + g_mean) / 3\n",
        "        green_mean = (r_mean + g_mean + b_mean) / 3\n",
        "        brightness = np.sqrt(0.299 * r_mean**2 + 0.587 * g_mean**2 + 0.114 * b_mean**2)\n",
        "\n",
        "        img = np.array([r_mean, g_mean, b_mean, nir_mean]).reshape((2, 2))\n",
        "        img = (img * 255).astype(np.uint8)  # Convert to uint8\n",
        "        glcm = graycomatrix(img, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256, symmetric=True, normed=True)\n",
        "        contrast = graycoprops(glcm, 'contrast').mean()\n",
        "        correlation = graycoprops(glcm, 'correlation').mean()\n",
        "        energy = graycoprops(glcm, 'energy').mean()\n",
        "        homogeneity = graycoprops(glcm, 'homogeneity').mean()\n",
        "\n",
        "        features2.append([r_mean, g_mean, b_mean, nir_mean, swir_mean, ndvi, ndwi, ndmi,\n",
        "                          red_mean, blue_mean, green_mean, brightness, contrast, correlation, energy, homogeneity])\n",
        "\n",
        "    return features2"
      ],
      "metadata": {
        "id": "jUGoJUj-nsIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Statistical Features for RGB and NIR and creating a dataframe\n",
        "features2 = extract_features(red_list, green_list, blue_list, nir_list, swir_list, ndvi_list, ndwi_list, ndmi_list)\n",
        "features2_data = pd.DataFrame(features2, columns = ['r_mean', 'g_mean', 'b_mean', 'nir_mean', 'swir_mean', 'ndvi', 'ndwi',\n",
        "                                                    'ndmi', 'red_mean', 'blue_mean', 'green_mean', 'brightness',\n",
        "                                                    'contrast', 'correlation', 'energy', 'homogeneity'])\n",
        "features2_data.head()"
      ],
      "metadata": {
        "id": "kQHE4FCrnup5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert NDVI and NDWI xarray to nparray\n",
        "ndvi_array = np.array([da for da in features2_data['ndvi']])\n",
        "ndvi_values = [da[0][0].item() for da in ndvi_array]\n",
        "ndvi = ndvi_values\n",
        "\n",
        "ndwi_array = np.array([da for da in features2_data['ndwi']])\n",
        "ndwi_values = [da[0][0].item() for da in ndwi_array]\n",
        "ndwi = ndwi_values\n",
        "\n",
        "ndmi_array = np.array([da for da in features2_data['ndmi']])\n",
        "ndmi_values = [da[0][0].item() for da in ndmi_array]\n",
        "ndmi = ndmi_values"
      ],
      "metadata": {
        "id": "6YhFfIPMnw7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame with updated columns\n",
        "features2_data['ndvi'] = ndvi\n",
        "features2_data['ndwi'] = ndwi\n",
        "features2_data['ndmi'] = ndmi\n",
        "features2_data.head()"
      ],
      "metadata": {
        "id": "Xmi6GSonny_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine two data"
      ],
      "metadata": {
        "id": "yByq46P4n3LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_two_datasets(dataset1,dataset2):\n",
        "    data = pd.concat([dataset1,dataset2], axis=1)\n",
        "    return data"
      ],
      "metadata": {
        "id": "qitS6XIqn0_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_data = combine_two_datasets(features_data, features2_data)\n",
        "features_data.head()"
      ],
      "metadata": {
        "id": "WSTwHJXjn747"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_data.to_csv('Features1_data.csv', index=False)"
      ],
      "metadata": {
        "id": "RXrIQE1cn9hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop_data = combine_two_datasets(crop_yield_data,features_data)\n",
        "crop_data.head()"
      ],
      "metadata": {
        "id": "pUXjfmZSn_B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take all columns of Features_data and Features2_data\n",
        "crop_data = crop_data[['min_vv', 'max_vv', 'range_vv', 'mean_vv', 'correlation_vv', 'permutation_entropy_vv',\n",
        "                       'min_vh', 'max_vh', 'range_vh', 'mean_vh', 'correlation_vh', 'permutation_entropy_vh',\n",
        "                       'min_vv_by_vh',  'max_vv_by_vh', 'range_vv_by_vh', 'mean_vv_by_vh', 'correlation_vv_by_vh',\n",
        "                       'permutation_entropy_vv_by_vh', 'rvi', 'backscatter_coefficient', 'polarization',\n",
        "\n",
        "                       'r_mean', 'g_mean', 'b_mean', 'nir_mean', 'swir_mean', 'ndvi', 'ndwi', 'ndmi',\n",
        "                       'red_mean', 'blue_mean', 'green_mean', 'brightness', 'contrast', 'correlation',\n",
        "                       'energy', 'homogeneity' 'Field size (ha)', 'Rice Yield (kg/ha)']]\n",
        "crop_data.head()"
      ],
      "metadata": {
        "id": "M0f7OvtGoA3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation matrix\n",
        "corrmat = crop_data.corr()\n",
        "f, ax = plt.subplots(figsize=(12, 9))\n",
        "sns.heatmap(corrmat, vmax=.8, square=True)"
      ],
      "metadata": {
        "id": "qVFcx2LuoCXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use columns correlated with Rice Yield\n",
        "crop_data = crop_data[['permutation_entropy_vv','permutation_entropy_vh','correlation_vv', 'correlation_vh',\n",
        "                       'permutation_entropy_vv_by_vh', 'correlation_vv_by_vh', 'rvi', 'backscatter_coefficient', 'polarization',\n",
        "\n",
        "                       'ndvi', 'ndwi', 'contrast', 'energy',\n",
        "\n",
        "                       'Rice Yield (kg/ha)']]\n",
        "crop_data.head()"
      ],
      "metadata": {
        "id": "Tj7j1u-OoEUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with all missing values in training and validation data\n",
        "crop_data = crop_data.dropna(axis=0, how='any')\n",
        "\n",
        "#Check if there is missing value\n",
        "missing_val_count_by_column = (crop_data.isnull().sum())\n",
        "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
      ],
      "metadata": {
        "id": "kpPfCN44oGOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features and target\n",
        "X = crop_data.drop('Rice Yield (kg/ha)', axis=1)\n",
        "y = crop_data['Rice Yield (kg/ha)']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(X_train_scaled.size)\n",
        "print(X_test_scaled.size)"
      ],
      "metadata": {
        "id": "1BAIsyXuoISs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhpK4PDsoMru"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}