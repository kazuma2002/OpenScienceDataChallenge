{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAPoEIJg3208/wvmoZ7c1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kazuma2002/OpenScienceDataChallenge/blob/main/Model_ricecropped.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX9kUVDzoYUr"
      },
      "outputs": [],
      "source": [
        "# Supress Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import ipyleaflet\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "import seaborn as sns\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Feature Engineering\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Planetary Computer Tools\n",
        "import pystac\n",
        "import pystac_client\n",
        "import odc\n",
        "from pystac_client import Client\n",
        "from pystac.extensions.eo import EOExtension as eo\n",
        "from odc.stac import stac_load\n",
        "import planetary_computer as pc\n",
        "\n",
        "#Please pass your API key here\n",
        "pc.settings.set_subscription_key('c3ed0e9c76f44014a77ef43b454f6747')\n",
        "\n",
        "# Others\n",
        "import requests\n",
        "import rich.table\n",
        "from itertools import cycle\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "tqdm_notebook.pandas()\n",
        "\n",
        "#Additionals\n",
        "!pip install mlxtend\n",
        "import multiprocessing\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "!pip install lightgbm\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crop_yield_data = pd.read_csv(\"Crop_Yield_Data_challenge_2.csv\")\n",
        "crop_yield_data.head()"
      ],
      "metadata": {
        "id": "5oTl7fWdoZ61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_data = pd.read_csv(\"Features1_data.csv\")\n",
        "features_data.head()"
      ],
      "metadata": {
        "id": "SznWz5KeobYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_two_datasets(dataset1,dataset2):\n",
        "    data = pd.concat([dataset1,dataset2], axis=1)\n",
        "    return data"
      ],
      "metadata": {
        "id": "TxRcR4LJoc3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop_data = combine_two_datasets(crop_yield_data,features_data)\n",
        "crop_data.head()"
      ],
      "metadata": {
        "id": "MnN2SmuFoeYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take all columns of Features_data and Features2_data\n",
        "crop_data = crop_data[['min_vv', 'max_vv', 'range_vv', 'mean_vv', 'correlation_vv', 'permutation_entropy_vv',\n",
        "                       'min_vh', 'max_vh', 'range_vh', 'mean_vh', 'correlation_vh', 'permutation_entropy_vh',\n",
        "                       'min_vv_by_vh',  'max_vv_by_vh', 'range_vv_by_vh', 'mean_vv_by_vh', 'correlation_vv_by_vh',\n",
        "                       'permutation_entropy_vv_by_vh', 'rvi', 'backscatter_coefficient', 'polarization',\n",
        "\n",
        "                       'r_mean', 'g_mean', 'b_mean', 'nir_mean', 'swir_mean', 'ndvi', 'ndwi', 'ndmi',\n",
        "                       'red_mean','blue_mean', 'green_mean', 'brightness', 'contrast', 'correlation',\n",
        "                       'energy', 'homogeneity', 'Field size (ha)', 'Rice Yield (kg/ha)']]\n",
        "crop_data.head()"
      ],
      "metadata": {
        "id": "5F6iqBXeogNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation matrix\n",
        "corrmat = crop_data.corr()\n",
        "f, ax = plt.subplots(figsize=(12, 9))\n",
        "sns.heatmap(corrmat, vmax=.8, square=True)"
      ],
      "metadata": {
        "id": "j4rsYkQhohyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use columns correlated with Rice Yield\n",
        "crop_data = crop_data[['ndvi', 'ndmi', 'brightness', 'homogeneity', 'correlation',\n",
        "                       'min_vv_by_vh',  'max_vv_by_vh', 'range_vv_by_vh', 'mean_vv_by_vh', 'correlation_vv_by_vh',\n",
        "                       'permutation_entropy_vv_by_vh', 'rvi', 'backscatter_coefficient', 'polarization',\n",
        "\n",
        "                       'Rice Yield (kg/ha)']]\n",
        "crop_data.head()"
      ],
      "metadata": {
        "id": "uhfthqmwojWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mstats\n",
        "def log_transform_data(data, columns):\n",
        "    log_transformed_data = data.copy()\n",
        "    for col in columns:\n",
        "        log_transformed_data[col] = np.log1p(log_transformed_data[col])\n",
        "    return log_transformed_data\n",
        "\n",
        "columns_to_log_transform = ['homogeneity', 'brightness', 'Field size (ha)']\n",
        "#crop_data = log_transform_data(crop_data, columns_to_log_transform)\n",
        "crop_data.describe().transpose()"
      ],
      "metadata": {
        "id": "d-SLmPfdok7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with all missing values in training and validation data\n",
        "crop_data = crop_data.dropna(axis=0, how='any')\n",
        "\n",
        "#Check if there is missing value\n",
        "missing_val_count_by_column = (crop_data.isnull().sum())\n",
        "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
      ],
      "metadata": {
        "id": "mGjEmugXomWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features and target\n",
        "X = crop_data.drop('Rice Yield (kg/ha)', axis=1)\n",
        "y = crop_data['Rice Yield (kg/ha)']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(X_train_scaled.size)\n",
        "print(X_test_scaled.size)"
      ],
      "metadata": {
        "id": "dC5tL_RNon2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the Random Forest model\n",
        "check_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "check_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = check_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Analyze feature importances\n",
        "importances = check_model.feature_importances_\n",
        "feature_importances = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
        "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
        "print(feature_importances)"
      ],
      "metadata": {
        "id": "-iDlaAgWopcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##GridSearch to find hyperparameter\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameters to search for each model\n",
        "dt_params = {\n",
        "    'max_depth': [5, 10, 15, 20],\n",
        "    'min_samples_leaf': [1, 2, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10, 15],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['log2', 'sqrt'],\n",
        "    'bootstrap' : [True, False]\n",
        "}\n",
        "gbr_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "svm_params = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'epsilon': [0.1, 0.2],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "nb_params = {\n",
        "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
        "}\n",
        "knn_params = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2]\n",
        "}\n",
        "xgb_params = {\n",
        "    'max_depth': [5, 10, 15, 20],\n",
        "    'min_child_weight': [1, 2, 5, 10],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.5, 0.75, 1],\n",
        "    'colsample_bytree': [0.5, 0.75, 1],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "lgb_params = {\n",
        "    'num_leaves': [31, 50, 100],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'num_iterations': [50, 100, 200],\n",
        "    'max_depth': [-1, 5, 10],\n",
        "    'min_data_in_leaf': [20, 50, 100]\n",
        "}\n",
        "et_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Create the four models\n",
        "dt_model = DecisionTreeRegressor()\n",
        "rf_model = RandomForestRegressor()\n",
        "gbr_model = GradientBoostingRegressor()\n",
        "svm_model = SVR()\n",
        "nb_model = GaussianNB()\n",
        "knn_model = KNeighborsRegressor()\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "lgb_model = lgb.LGBMRegressor(objective='regression', metric='rmse')\n",
        "et_model = ExtraTreesRegressor()\n",
        "\n",
        "# Define the parameter grid for each model\n",
        "dt_grid = GridSearchCV(dt_model, dt_params, cv=5, n_jobs= -1)\n",
        "rf_grid = GridSearchCV(rf_model, rf_params, cv=5, n_jobs= -1)\n",
        "gbr_grid = GridSearchCV(gbr_model, gbr_params, cv=5, n_jobs= -1)\n",
        "svm_grid = GridSearchCV(svm_model, svm_params, cv=5, n_jobs= -1)\n",
        "nb_grid = GridSearchCV(nb_model, nb_params, cv=5, n_jobs= -1)\n",
        "knn_grid = GridSearchCV(knn_model, knn_params, cv=5, n_jobs=-1)\n",
        "xgb_grid = GridSearchCV(xgb_model, xgb_params, cv=5, n_jobs=-1)\n",
        "lgb_grid = GridSearchCV(lgb_model, lgb_params, cv=5, n_jobs=-1)\n",
        "et_grid = GridSearchCV(et_model, et_params, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the grids to the training data with a progress bar\n",
        "for grid in tqdm([dt_grid, rf_grid, gbr_grid, nb_grid, knn_grid, xgb_grid, et_grid, lgb_grid]):\n",
        "    grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "clcoQtTNoq8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameters for each model\n",
        "print(\"Decision Tree best params:\", dt_grid.best_params_)\n",
        "print(\"Random Forest best params:\", rf_grid.best_params_)\n",
        "print(\"Gradient Boosting Regressor best params:\", gbr_grid.best_params_)\n",
        "print(\"Naive Bayes best params:\", nb_grid.best_params_)\n",
        "print(\"K-nearest Regressor best params:\", knn_grid.best_params_)\n",
        "print(\"XGBoost best params:\", xgb_grid.best_params_)\n",
        "print(\"Light GBM best params:\", lgb_grid.best_params_)\n",
        "print(\"Extra Tree best params:\", et_grid.best_params_)"
      ],
      "metadata": {
        "id": "Gf6SdhsAosYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the four models on the training data\n",
        "dt_model = DecisionTreeRegressor(max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=2)\n",
        "dt_cv_scores = cross_val_score(dt_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "dt_cv_rmse = np.mean(np.abs(dt_cv_scores))\n",
        "dt_model.fit(X_train, y_train)\n",
        "rf_model = RandomForestRegressor(bootstrap=True, max_depth=10, max_features='log2',\n",
        "                                 min_samples_leaf=1, min_samples_split=5, n_estimators=50)\n",
        "rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "rf_cv_rmse = np.mean(np.abs(rf_cv_scores))\n",
        "rf_model.fit(X_train, y_train)\n",
        "gbr_model = GradientBoostingRegressor(learning_rate=0.01, max_depth=5, max_features=None,\n",
        "                                      min_samples_leaf=2, min_samples_split=5, n_estimators=200)\n",
        "gbr_cv_scores = cross_val_score(gbr_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "gbr_cv_rmse = np.mean(np.abs(gbr_cv_scores))\n",
        "gbr_model.fit(X_train, y_train)\n",
        "nb_model = GaussianNB(var_smoothing=1e-07)\n",
        "nb_cv_scores = cross_val_score(nb_model, X_train_scaled, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "nb_cv_rmse = np.mean(np.abs(nb_cv_scores))\n",
        "nb_model.fit(X_train_scaled, y_train)\n",
        "knn_model = KNeighborsRegressor(n_neighbors=9, p=1, weights='uniform')\n",
        "knn_cv_scores = cross_val_score(knn_model, X_train_scaled, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "knn_cv_rmse = np.mean(np.abs(knn_cv_scores))\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "xgb_model = xgb.XGBRegressor(colsample_bytree=0.5, learning_rate=0.05, max_depth=5,\n",
        "                             min_child_weight=5, n_estimators=100, subsample=1)\n",
        "xgb_cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "xgb_cv_rmse = np.mean(np.abs(xgb_cv_scores))\n",
        "xgb_model.fit(X_train, y_train)\n",
        "lgb_model = lgb.LGBMRegressor(learning_rate=0.05, max_depth=-5, min_data_in_leaf=20, num_iterations=50,\n",
        "                              num_leaves=31, verbosity=-1)\n",
        "lgb_cv_scores = cross_val_score(lgb_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "lgb_cv_rmse = np.mean(np.abs(lgb_cv_scores))\n",
        "lgb_model.fit(X_train, y_train)\n",
        "et_model = ExtraTreesRegressor(max_depth=10, max_features='auto', n_estimators=200)\n",
        "et_cv_scores = cross_val_score(et_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "et_cv_rmse = np.mean(np.abs(et_cv_scores))\n",
        "et_model.fit(X_train, y_train)\n",
        "svm_model = SVR(C=10, epsilon=0.1, gamma='scale', kernel='linear')\n",
        "svm_cv_scores = cross_val_score(svm_model, X_train_scaled, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "svm_cv_rmse = np.mean(np.abs(svm_cv_scores))\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "dt_pred = dt_model.predict(X_test)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "gbr_pred = gbr_model.predict(X_test)\n",
        "nb_pred = nb_model.predict(X_test)\n",
        "knn_pred = knn_model.predict(X_test)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "lgb_pred = lgb_model.predict(X_test)\n",
        "svm_pred = svm_model.predict(X_test)\n",
        "et_pred = et_model.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Cross-validation RMSE:\", dt_cv_rmse)\n",
        "print(\"Random Forest Cross-validation RMSE:\", rf_cv_rmse)\n",
        "print(\"Gradient Boosting Cross-validation RMSE:\", gbr_cv_rmse)\n",
        "print(\"Naive Bayes Cross-validation RMSE:\", nb_cv_rmse)\n",
        "print(\"Extra Tree Cross-validation RMSE:\", et_cv_rmse)\n",
        "print(\"K-Nearest Neighbors Cross-validation RMSE:\", knn_cv_rmse)\n",
        "print(\"XGboost Cross-validation RMSE:\", xgb_cv_rmse)\n",
        "print(\"Light GBM Cross-validation RMSE:\", lgb_cv_rmse)\n",
        "print(\"SVM Cross-validation RMSE:\", svm_cv_rmse)"
      ],
      "metadata": {
        "id": "X27KqgS0ouOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_r2 = r2_score(y_test, dt_pred)\n",
        "rf_r2 = r2_score(y_test, rf_pred)\n",
        "gbr_r2 = r2_score(y_test, gbr_pred)\n",
        "nb_r2 = r2_score(y_test, nb_pred)\n",
        "knn_r2 = r2_score(y_test, knn_pred)\n",
        "xgb_r2 = r2_score(y_test, xgb_pred)\n",
        "lgb_r2 = r2_score(y_test, lgb_pred)\n",
        "svm_r2 = r2_score(y_test, dt_pred)\n",
        "et_r2 = r2_score(y_test, et_pred)\n",
        "print(\"Decision Tree R2:\", dt_r2)\n",
        "print(\"Random Forest R2:\", rf_r2)\n",
        "print(\"Gradient Boosting R2:\", gbr_r2)\n",
        "print(\"Naive Bayes R2:\", nb_r2)\n",
        "print(\"Extra Tree R2:\", et_r2)\n",
        "print(\"K-Nearest Neighbors R2:\", knn_r2)\n",
        "print(\"XGboost R2:\", xgb_r2)\n",
        "print(\"Light GBM R2:\", lgb_r2)\n",
        "print(\"SVM R2:\", svm_r2)"
      ],
      "metadata": {
        "id": "8Gic6UJaowgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make another data set for another aspect\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "# Fit the grids to the training data with a progress bar\n",
        "for grid in tqdm([dt_grid, rf_grid, gbr_grid, et_grid, xgb_grid, lgb_grid]):\n",
        "    grid.fit(X2_train, y2_train)"
      ],
      "metadata": {
        "id": "XwEMfjRRo0xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameters for each model\n",
        "print(\"Decision Tree best params:\", dt_grid.best_params_)\n",
        "print(\"Random Forest best params:\", rf_grid.best_params_)\n",
        "print(\"Gradient Boosting Regressor best params:\", gbr_grid.best_params_)\n",
        "print(\"XGBoost best params:\", xgb_grid.best_params_)\n",
        "print(\"Light GBM best params:\", lgb_grid.best_params_)\n",
        "print(\"Extra Tree best params:\", et_grid.best_params_)"
      ],
      "metadata": {
        "id": "vdrB95ieo2K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the four models on the training data\n",
        "dt2_model = DecisionTreeRegressor(max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=5)\n",
        "dt2_cv_scores = cross_val_score(dt2_model, X2_train, y2_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "dt2_cv_rmse = np.mean(np.abs(dt2_cv_scores))\n",
        "dt2_model.fit(X2_train, y2_train)\n",
        "rf2_model = RandomForestRegressor(bootstrap=False, max_depth=5, max_features='sqrt',\n",
        "                                 min_samples_leaf=1, min_samples_split=2, n_estimators=200)\n",
        "rf2_cv_scores = cross_val_score(rf2_model, X2_train, y2_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "rf2_cv_rmse = np.mean(np.abs(rf2_cv_scores))\n",
        "rf2_model.fit(X2_train, y2_train)\n",
        "gbr2_model = GradientBoostingRegressor(learning_rate=0.1, max_depth=3, max_features='sqrt',\n",
        "                                      min_samples_leaf=4, min_samples_split=10, n_estimators=50)\n",
        "gbr2_cv_scores = cross_val_score(gbr2_model, X2_train, y2_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "gbr2_cv_rmse = np.mean(np.abs(gbr2_cv_scores))\n",
        "gbr2_model.fit(X2_train, y2_train)\n",
        "xgb2_model = xgb.XGBRegressor(colsample_bytree=0.75, learning_rate=0.1, max_depth=5,\n",
        "                             min_child_weight=2, n_estimators=50, subsample=1)\n",
        "xgb2_cv_scores = cross_val_score(xgb2_model, X2_train, y2_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "xgb2_cv_rmse = np.mean(np.abs(xgb2_cv_scores))\n",
        "xgb2_model.fit(X2_train, y2_train)\n",
        "lgb2_model = lgb.LGBMRegressor(learning_rate=0.05, max_depth=5, min_data_in_leaf=20, num_iterations=50,\n",
        "                               num_leaves=31, verbosity=-1)\n",
        "lgb2_cv_scores = cross_val_score(lgb2_model, X2_train, y2_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "lgb2_cv_rmse = np.mean(np.abs(lgb2_cv_scores))\n",
        "lgb2_model.fit(X2_train, y2_train)\n",
        "et2_model = ExtraTreesRegressor(max_depth=10, max_features='auto', n_estimators=200)\n",
        "et2_cv_scores = cross_val_score(et2_model, X2_train, y2_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "et2_cv_rmse = np.mean(np.abs(et2_cv_scores))\n",
        "et2_model.fit(X2_train, y2_train)\n",
        "\n",
        "print(\"Decision Tree2 Cross-validation RMSE:\", dt2_cv_rmse)\n",
        "print(\"Random Forest2 Cross-validation RMSE:\", rf2_cv_rmse)\n",
        "print(\"Gradient Boosting2 Cross-validation RMSE:\", gbr2_cv_rmse)\n",
        "print(\"XGboost2 Cross-validation RMSE:\", xgb2_cv_rmse)\n",
        "print(\"Light GBM2 Cross-validation RMSE:\", lgb2_cv_rmse)\n",
        "print(\"Extra Tree2 Cross-validation RMSE:\", et2_cv_rmse)"
      ],
      "metadata": {
        "id": "GP57nCN4o3nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_r2 = r2_score(y_test, dt_pred)\n",
        "rf_r2 = r2_score(y_test, rf_pred)\n",
        "gbr_r2 = r2_score(y_test, gbr_pred)\n",
        "xgb_r2 = r2_score(y_test, xgb_pred)\n",
        "lgb_r2 = r2_score(y_test, lgb_pred)\n",
        "et_r2 = r2_score(y_test, et_pred)\n",
        "print(\"Decision Tree R2:\", dt_r2)\n",
        "print(\"Random Forest R2:\", rf_r2)\n",
        "print(\"Gradient Boosting R2:\", gbr_r2)\n",
        "print(\"Extra Tree R2:\", et_r2)\n",
        "print(\"XGboost R2:\", xgb_r2)\n",
        "print(\"Light GBM R2:\", lgb_r2)"
      ],
      "metadata": {
        "id": "lj62esl-o5C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a stack model\n",
        "lr_model = LinearRegression()\n",
        "stack_model = StackingCVRegressor(regressors=(lgb_model,gbr_model,rf_model,et_model,xgb_model,\n",
        "                                              rf2_model, gbr2_model, xgb2_model, lgb2_model, et2_model),\n",
        "                                  meta_regressor=lr_model, cv=KFold(n_splits=5, shuffle=True, random_state=21))\n",
        "\n",
        "sm_cv_scores = cross_val_score(stack_model, X4_train, y4_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "sm_cv_rmse = np.mean(np.abs(sm_cv_scores))\n",
        "\n",
        "stack_model.fit(X4_train, y4_train)\n",
        "stack_pred = stack_model.predict(X4_test)\n",
        "stack_r2 = r2_score(y4_test, stack_pred)"
      ],
      "metadata": {
        "id": "BD-uyHtFo7Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stacking R2Score:\", stack_r2)\n",
        "print(\"Cross-validation RMSE:\", sm_cv_rmse)"
      ],
      "metadata": {
        "id": "El4S9JjxpERE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAbiokYwpGAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}