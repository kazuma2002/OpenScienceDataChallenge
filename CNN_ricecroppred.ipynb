{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTm0y6XHNHe9K6RUx5yXiB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kazuma2002/OpenScienceDataChallenge/blob/main/CNN_ricecroppred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_UEOIrIpYPR"
      },
      "outputs": [],
      "source": [
        "# Supress Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import ipyleaflet\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "import seaborn as sns\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Feature Engineering\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Planetary Computer Tools\n",
        "import pystac\n",
        "import pystac_client\n",
        "import odc\n",
        "from pystac_client import Client\n",
        "from pystac.extensions.eo import EOExtension as eo\n",
        "from odc.stac import stac_load\n",
        "import planetary_computer as pc\n",
        "\n",
        "#Please pass your API key here\n",
        "pc.settings.set_subscription_key('c3ed0e9c76f44014a77ef43b454f6747')\n",
        "\n",
        "# Others\n",
        "import requests\n",
        "import rich.table\n",
        "from itertools import cycle\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "tqdm_notebook.pandas()\n",
        "\n",
        "#Additionals\n",
        "!pip install mlxtend\n",
        "import multiprocessing\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crop_yield_data = pd.read_csv(\"Crop_Yield_Data_challenge_2.csv\")\n",
        "crop_yield_data.head()"
      ],
      "metadata": {
        "id": "MCTrfPgHpfGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_data = pd.read_csv(\"Features1_data.csv\")\n",
        "features_data.head()"
      ],
      "metadata": {
        "id": "-RFm6Ny2pgs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_two_datasets(dataset1,dataset2):\n",
        "    data = pd.concat([dataset1,dataset2], axis=1)\n",
        "    return data"
      ],
      "metadata": {
        "id": "CFa5cj_8ph6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop_data = combine_two_datasets(crop_yield_data,features_data)\n",
        "crop_data.head()"
      ],
      "metadata": {
        "id": "wKvlKSJMpjZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take all columns of Features_data and Features2_data\n",
        "crop_data = crop_data[['min_vv', 'max_vv', 'range_vv', 'mean_vv', 'correlation_vv', 'permutation_entropy_vv',\n",
        "                       'min_vh', 'max_vh', 'range_vh', 'mean_vh', 'correlation_vh', 'permutation_entropy_vh',\n",
        "                       'min_vv_by_vh',  'max_vv_by_vh', 'range_vv_by_vh', 'mean_vv_by_vh', 'correlation_vv_by_vh',\n",
        "                       'permutation_entropy_vv_by_vh', 'rvi', 'backscatter_coefficient', 'polarization',\n",
        "\n",
        "                       'r_mean', 'g_mean', 'b_mean', 'nir_mean', 'swir_mean', 'ndvi', 'ndwi', 'ndmi',\n",
        "                       'red_mean','blue_mean', 'green_mean', 'brightness', 'contrast', 'correlation',\n",
        "                       'energy', 'homogeneity', 'Field size (ha)', 'Rice Yield (kg/ha)']]\n",
        "crop_data.head()"
      ],
      "metadata": {
        "id": "t1_aTgQeprbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation matrix\n",
        "corrmat = crop_data.corr()\n",
        "f, ax = plt.subplots(figsize=(12, 9))\n",
        "sns.heatmap(corrmat, vmax=.8, square=True)"
      ],
      "metadata": {
        "id": "iNttBO12ptnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use columns correlated with Rice Yield\n",
        "crop_data = crop_data[['permutation_entropy_vv','permutation_entropy_vh','correlation_vv', 'correlation_vh',\n",
        "                       'permutation_entropy_vv_by_vh', 'correlation_vv_by_vh', 'rvi', 'backscatter_coefficient', 'polarization',\n",
        "                       'ndvi', 'ndwi', 'ndmi', 'brightness', 'contrast', 'correlation',\n",
        "                       'energy', 'homogeneity', 'Field size (ha)',\n",
        "\n",
        "                       'Rice Yield (kg/ha)']]\n",
        "crop_data.head()"
      ],
      "metadata": {
        "id": "RYQdDdSYpvJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of values for a specific column\n",
        "def plot_column_distribution(data, column):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    sns.boxplot(data=data[column], ax=ax1)\n",
        "    ax1.set_title(f'Box plot of {column}')\n",
        "\n",
        "    sns.histplot(data=data[column], kde=True, ax=ax2)\n",
        "    ax2.set_title(f'Histogram and KDE of {column}')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# List of feature columns\n",
        "feature_columns = ['permutation_entropy_vv','permutation_entropy_vh','correlation_vv', 'correlation_vh',\n",
        "                       'permutation_entropy_vv_by_vh', 'correlation_vv_by_vh', 'rvi', 'backscatter_coefficient', 'polarization',\n",
        "                       'ndvi', 'ndwi', 'ndmi', 'brightness', 'contrast', 'correlation',\n",
        "                       'energy', 'homogeneity', 'Field size (ha)',\n",
        "\n",
        "                       'Rice Yield (kg/ha)']\n",
        "\n",
        "# Visualize the distribution for all feature columns\n",
        "for column in feature_columns:\n",
        "    plot_column_distribution(crop_data, column)"
      ],
      "metadata": {
        "id": "isRcPsropwkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with all missing values in training and validation data\n",
        "crop_data = crop_data.dropna(axis=0, how='any')\n",
        "\n",
        "#Check if there is missing value\n",
        "missing_val_count_by_column = (crop_data.isnull().sum())\n",
        "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
      ],
      "metadata": {
        "id": "5sAki4kRpyId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mstats\n",
        "def winsorize_data(data, columns, limits=(0.05, 0.05)):\n",
        "    winsorized_data = data.copy()\n",
        "    for col in columns:\n",
        "        winsorized_data[col] = mstats.winsorize(winsorized_data[col], limits=limits)\n",
        "    return winsorized_data\n",
        "\n",
        "columns_to_winsorize = ['homogeneity', 'correlation', 'brightness', 'contrast', 'Field size (ha)']\n",
        "winsorized_data = winsorize_data(crop_data, columns_to_winsorize)\n",
        "\n",
        "def log_transform_data(data, columns):\n",
        "    log_transformed_data = data.copy()\n",
        "    for col in columns:\n",
        "        log_transformed_data[col] = np.log1p(log_transformed_data[col])\n",
        "    return log_transformed_data\n",
        "\n",
        "columns_to_log_transform = ['homogeneity', 'correlation', 'brightness', 'contrast', 'Field size (ha)']\n",
        "log_transformed_data = log_transform_data(crop_data, columns_to_log_transform)"
      ],
      "metadata": {
        "id": "LcUrSoRbpzgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_transformed_data.describe().transpose()"
      ],
      "metadata": {
        "id": "P9rfA-g-p06x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop_data = log_transformed_data"
      ],
      "metadata": {
        "id": "dOsAOsyvp3o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features and target\n",
        "X = crop_data.drop('Rice Yield (kg/ha)', axis=1)\n",
        "y = crop_data['Rice Yield (kg/ha)']\n",
        "\n",
        "# Convert pandas DataFrames to NumPy arrays\n",
        "X = X.values\n",
        "y = y.values\n",
        "\n",
        "# Reshape input data for the FCN\n",
        "X = X.reshape(X.shape[0], X.shape[1])\n",
        "\n",
        "# Scale input data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.size)\n",
        "print(X_test.size)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "KurZzYIWp6Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the Random Forest model\n",
        "check_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "check_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = check_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Analyze feature importances\n",
        "importances = check_model.feature_importances_\n",
        "feature_names = np.array(['permutation_entropy_vv','permutation_entropy_vh','correlation_vv', 'correlation_vh',\n",
        "                       'permutation_entropy_vv_by_vh', 'correlation_vv_by_vh', 'rvi', 'backscatter_coefficient', 'polarization',\n",
        "                       'ndvi', 'ndwi', 'ndmi', 'brightness', 'contrast', 'correlation',\n",
        "                       'energy', 'homogeneity', 'Field size (ha)',])\n",
        "feature_importances = np.column_stack((feature_names, importances))\n",
        "sorted_idx = np.argsort(importances)[::-1]\n",
        "sorted_feature_importances = feature_importances[sorted_idx]\n",
        "\n",
        "print(sorted_feature_importances)"
      ],
      "metadata": {
        "id": "W6q2pTm_p7f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CNN model\n",
        "def create_cnn(input_shape):\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(256, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(512, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create FCN model\n",
        "def create_fcn(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, activation='relu', input_shape=(input_shape,)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "pdIv6jHMp86s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that creates the model with the specified learning rate\n",
        "def create_model(learning_rate=0.001):\n",
        "    model = create_cnnn(input_shape) # cnn or fcn\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "# Create the KerasRegressor wrapper\n",
        "model = KerasRegressor(build_fn=create_model, epochs=100, verbose=0)\n",
        "\n",
        "# Define the hyperparameter grid or distribution\n",
        "param_dist = {\n",
        "    'learning_rate': np.logspace(-5, -2, 30),\n",
        "    'batch_size': [8, 16, 32, 64, 128]\n",
        "}\n",
        "\n",
        "# Create the random search object\n",
        "input_shape = 14 # Check the data shape (18 columns)\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model, param_distributions=param_dist, n_iter=20, cv=3, verbose=2, n_jobs=-1\n",
        ")\n",
        "\n",
        "# Run the random search\n",
        "random_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "c9gOjSaKp-_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameters\n",
        "print(\"Best parameters found: \", random_search.best_params_)"
      ],
      "metadata": {
        "id": "IBYt7Xb4qBcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a model\n",
        "input_shape = 18\n",
        "model = create_fcn(input_shape) # cnn or fcn\n",
        "\n",
        "# Compile the model\n",
        "learning_rate = 0.0025  # Adjust this value\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "z1CowBG-qEEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_GSX92AqFch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}